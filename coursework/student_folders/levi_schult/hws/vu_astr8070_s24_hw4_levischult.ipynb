{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASTR 8070: Astrostatistics\n",
    "***S. R. Taylor***\n",
    "___\n",
    "\n",
    "# Homework 4\n",
    "### Due: Saturday, Feb 24th at 11.59pm CST\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "This problem uses a dataset in `/coursework/homeworks/hw_data/`.\n",
    "\n",
    "1) Read in `hw4_data_1.npy`. This is a (10 x 2) numpy array, with voltage measurements in the first column and heteroscedastic voltage uncertainties in the second column. Compute the sample mean and the standard error on the sample mean for this data.\n",
    "\n",
    "2) Fit the appropriate ln-likelihood function and find the best-fit mean voltage.\n",
    "\n",
    "3) Compute and plot the Bayesian posterior probability density (*not the log posterior*) for the mean voltage assuming a uniform prior for the mean in the range 3 to 7. Make sure this posterior pdf is normalized!\n",
    "\n",
    "4) By either drawing samples from this posterior, or using your gridded posterior pdf to make a cdf, find the equal-tailed 68.3% credible region for the mean, and compare the upper and lower boundaries to the sample mean plus/minus the standard error, respectively. *Also* find the MAP value of the mean.\n",
    "\n",
    "5) Repeat (3) and (4) this time with a prior on the mean that is uniform in the range 4.6 to 5.4. \n",
    "\n",
    "6) Now, imagine that we read an old paper about the experiment that gave us the voltage measurements, and they found that the mean was actually $6\\pm0.3$. Repeat (3) and (4) this time with a Gaussian prior on the mean centered at $6$ with standard deviation of $0.3$.\n",
    "\n",
    "7) Plot all of the normalized posterior pdfs for $\\mu$ from (3), (5), and (6) on the same plot, making sure that the xlim of the plot spans 0 to 10.\n",
    "\n",
    "8) You have made sure that the posterior pdfs are properly normalized, but until now you have ignored the meaning of that normalization constant. It is the Bayesian evidence for the particular model you have applied! Compute the evidence under a new model where the prior for the mean is a delta function at the best-fit value you found in (1) *(think about this and don't just immediately go looking for a `scipy.stats` delta function)*. Compare this to the evidence found under the prior in (3). Taking ratios to make a Bayes factor, which model is favored? Is there much of an Occam penalty by having the wide prior compared to knowing the mean exactly? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "from matplotlib.offsetbox import AnchoredOffsetbox, TextArea, HPacker, VPacker\n",
    "import chainconsumer\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSS loading data\n",
    "data1 = np.load('../../../homeworks/hw_data/hw4_data_1.npy')\n",
    "\n",
    "def het_mean_sigma_analytic(data):\n",
    "    '''\n",
    "    A function to analytically calculate the mean and uncert on mean for \n",
    "    a heteroscedastic dataset\n",
    "\n",
    "    inputs:\n",
    "    data (ndarray): N measurements x 2 array where [:, 1] are the uncertainties\n",
    "\n",
    "    returns:\n",
    "    mean (float): mean analytically calculated for heteroscedastic data\n",
    "    sigmu (float): uncertainty on the mean analytically calc.\n",
    "    '''\n",
    "    numerator = np.sum(data[:, 0] / data[:, 1]**2) # LSS sum of measurement over\n",
    "    # the square of the respective uncertainties\n",
    "    denom = np.sum(data[:, 1]**-2) # LSS sum of the inverse square of the uncert.\n",
    "\n",
    "    mean = numerator / denom\n",
    "\n",
    "    sigmu = denom**-0.5 # LSS uncert on mean is denominator to the -0.5\n",
    "    return mean, sigmu\n",
    "\n",
    "data1mu_an, data1sig_an = het_mean_sigma_analytic(data1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.942118214425304"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1mu_an"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neglnlik(mu_model, data, uncert, modelname='gaussian', scaleparam=np.nan):\n",
    "    '''\n",
    "    A function to return the negative log likelihood for data when modeled by a \n",
    "    gaussian with mu=mu_model\n",
    "\n",
    "    inputs:\n",
    "    mu_model (float): mean for the gaussian model\n",
    "    data (ndarray): the data to be fit to \n",
    "    uncert (ndarray): the uncert for each data point\n",
    "    outputs:\n",
    "    neglnlik (float): negative log likelihood for some mu_model \n",
    "    '''\n",
    "    if modelname=='gaussian':\n",
    "        # LSS calculating likelihood\n",
    "        neglnlik = np.sum(np.log(1/(uncert*np.sqrt(2*np.pi))) - ((data - mu_model)**2 / (2*uncert**2)))\n",
    "    \n",
    "    elif modelname=='laplace': # LSS encoding laplace model\n",
    "        if np.isnan(scaleparam):\n",
    "            print('MUST ASSIGN SCALE PARAMETER IF USING LAPLACE MODEL')\n",
    "            return\n",
    "        neglnlik = np.sum(np.log(1/(2*scaleparam))+-(np.abs(data-mu_model)/scaleparam))\n",
    "\n",
    "    return -neglnlik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 16.048013\n",
      "         Iterations: 17\n",
      "         Function evaluations: 34\n"
     ]
    }
   ],
   "source": [
    "# LSS testing neglnlik function\n",
    "neglnlik(4, data1[:, 0], data1[:, 1])\n",
    "\n",
    "# LSS finding max ln lik (min negative)\n",
    "nlnlik_mu1guess = lambda muguess: neglnlik(muguess, data=data1[:, 0], \n",
    "                                          uncert=data1[:, 1])\n",
    "muguess0 = 3\n",
    "mu1_MLE = optimize.fmin(nlnlik_mu1guess, muguess0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE estimate: 4.942089843750004\n",
      "analytic est: 4.942118214425304\n"
     ]
    }
   ],
   "source": [
    "print(f'MLE estimate: {mu1_MLE[0]}')\n",
    "print(f'analytic est: {data1mu_an}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "This problem uses a dataset in `/coursework/homeworks/hw_data/`.\n",
    "\n",
    "1) Read in `hw4_data_2.npy`, which is a (3 x 20) numpy array that you used in `Lecture_9`. Set `x, y, sigma_y = data`. \n",
    "\n",
    "We're going to do some polynomial fits to this data just like in `Lecture 9`. However, in all cases you should **keep the $y$-intercept fixed at $-0.23$**. \n",
    "\n",
    "2) Use the following code to compute the un-normalized posterior pdf (i.e. just the likelihood x prior) on a grid of the linear coefficient (i.e. the slope) of a linear model, with a uniform prior between 0.5 and 1.5. Plot this posterior pdf. Remember this is just a one-dimensional model because the $y$-intercept is fixed. I advise a grid size of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to do a polynomial fit, and compute the likelihood\n",
    "def polynomial_fit(theta, x):\n",
    "    \"\"\"Polynomial model of degree (len(theta) - 1)\"\"\"\n",
    "    # For a polynomial with order 1, this gives theta_0 + theta_1*x\n",
    "    # For a polynomial with order 2, this gives theta_0 + theta_1*x + theta_2*x^2, etc.\n",
    "    return sum(t * x ** n for (n, t) in enumerate(theta))\n",
    "\n",
    "# compute the data log-likelihood given a model\n",
    "def logL(theta, data, model=polynomial_fit):\n",
    "    \"\"\"Gaussian log-likelihood of the model at theta\"\"\"\n",
    "    x, y, sigma_y = data\n",
    "    y_fit = model(theta, x)\n",
    "    return sum(scipy.stats.norm.logpdf(*args) \n",
    "               for args in zip(y, y_fit, sigma_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Using your 1D gridded likelihood-x-prior, compute the Bayesian evidence of this linear model. This may be a big number!\n",
    "\n",
    "4) Now compute the joint 2D posterior pdf (again just the likelihood x prior) of linear and quadratic coefficients of a quadratic model. Give the linear coefficient a uniform prior between 0.5 and 1.5. Give the quadratic coefficient a uniform prior between -1 and 0.25. Plot this two-dimensional posterior. Remember this is a two-dimensional model because the $y$-intercept is fixed. I advise a grid size of 100 in each model dimension.\n",
    "\n",
    "5) Using your 2D gridded likelihood-x-prior, compute the Bayesian evidence of the quadratic model. \n",
    "\n",
    "6) Calculate the Bayes factor for a linear versus quadratic model. How does this compare/contrast with the BIC model comparison in the lecture? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
